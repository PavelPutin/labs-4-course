%%% Пример переделки лабы 3 (ГСВ с различными ков. матрицами) на svm.(?!^)\s{2,}
% Получение экспериментальной матрицы ошибок по svm
% Пример для 3х классов - сравнение каждый с остальными.

% clear all;
close all;

%% 1. Задание исходных данных
n = 2; % размерность признакового пространства
M = 3; % число классов
K = 200; % количество статистических испытаний

% Априорные вероятности, математические ожидания и матрицы ковариации классов
dm = 2.0; %  расстояние между математическими ожиданиями классов по координатным осям
C = zeros(n, n, M); C_ = C; %  матрица ковариации вектора признаков различных классов
pw = [0.2 0.3 0.5];
% pw = [0.4 0.6]; % для двух классов (M = 2)
pw = pw / sum(pw);
D = 3 * eye(2);
m = [3 3; -3 -3; -3 3]';
% m = [-3 -3; 3 3]'; % для двух классов (M = 2)
C(:, :, 1) = [3 -1; -1 3];
C(:, :, 2) = [5 3; 3 5];
C(:, :, 3) = [5 -3; -3 5];
for k = 1 : M
    C_(:, :, k) = C(:, :, k) ^ - 1; 
end
np = sum(pw); pw = pw / np; % исключение некорректного задания априорных вероятностей

%% 2. Обучение svm классификаторов для каждого класса
% 2.1. Генерация обучающих выборок классов
% Объемы выборок каждого класса
Ks = fix(K * pw);
Ks(end) = K - sum(Ks(1 : end - 1));
X = []; % общая обучающая выборка (все образы всех классов)
Y = []; % номера классов для каждого образа
% Генерация выборок

for i = 1 : M % цикл по классам
    XN{i} = repmat(m(:, i), [1, Ks(i)]) + randncor(n, Ks(i), C(:, :, i)); % генерация Ks(i) образов i-го класса
    X = cat(1, X, XN{i}'); % помещаем образы в общую выборку
    Y = cat(1, Y, i * ones(Ks(i), 1)); %  номер класса для каждого образа
end

%% 2.2. Сначала обучаем классификаторы, чтобы на эксперименте дёргать уже обученные.
% Поскольку классов может быть 3, а svm осуществляет только попарное
% сравнение, то обучаем свой классификатор для каждого класса с остальными
% (для сравнения 1го и 2го+3го, 2го и 1го+3го, 3го и 1го+2го классов)

% Обучаем svm-классификаторы для каждого класса
r = 1; % параметр регуляризации (балансирует положение разделяющей границы относительно положения опорных векторов и "объему" заступов за границу)
svm_strs = cell(1, M);
for i = 1 : M % цикл классам
    D = Y == i; % метки классов
	
% Вар. а) Классификатор для линейно разделимых данных (разделяющая граница - прямая)
%     svm_strs{i} = fitcsvm(X, D, 'Solver', 'L1QP', 'KernelFunction', 'linear', 'BoxConstraint', r);
    
% Вар. б) Классификатор для линейно неразделимых данных (разделяющая граница - кривая)
%     svm_strs{i} = fitcsvm(X, D, 'Solver', 'L1QP', 'KernelFunction', 'polynomial', 'PolynomialOrder', 4);
% Другие варианты Kernel_Function и её параметров (смотри файл fitcsvm.m) 
    svm_strs{i} = fitcsvm(X, D, 'Solver', 'L1QP', 'KernelFunction', 'rbf', 'KernelScale', 1);
end

%% 2.3. Отображение областей локализации классов
show = true; % визуализация результатов обучения (если n = 3, можно установить false - 3d пространство не визуализируется)
if show    
    % 2.3.1 Формируем дискретную двумерную сетку отсчётов
    d = 0.05; % шаг сетки отсчётов
    [x1Grid, x2Grid] = meshgrid(min(X(:, 1)) : d : max(X(:, 1)), ...
        min(X(:, 2)) : d : max(X(:, 2)));
    xGrid = [x1Grid(:), x2Grid(:)];
    N = size(xGrid, 1);
    Scores = []; % апостериорные плотности классов
    sv = []; % массив опорных векторов
    
    % 2.3.2 Классифицируем узлы дискретной сетки
    for i = 1 : M
        sv = cat(1, sv, svm_strs{i}.SupportVectors); % опорные векторы
        [~, score] = predict(svm_strs{i}, xGrid); % получаем вероятность принадлежности узла к одному из классов
        Scores = cat(2, Scores, score(:, 2)); % второй столбец - апостериорная плотность i-го класса
    end
    [~, maxScore] = max(Scores, [], 2);

    % 2.3.3 Отрисовка узлов двумерной сетки разными цветами
	figure; % создаём графическое окно
    if M == 2
        h(1 : M) = gscatter(xGrid(:, 1), xGrid(:, 2), maxScore, [0.5 0.1 0.5; 0.1 0.5 0.5]);
    elseif M == 3
        h(1 : M) = gscatter(xGrid(:, 1), xGrid(:, 2), maxScore, [0.5 0.1 0.5; 0.5 0.5 0.1; 0.1 0.5 0.5]);
    end
    hold on
    
    % 2.3.4 Отрисовка образов обучающих выборок
    h(M+1 : 2 * M) = gscatter(X(:, 1), X(:, 2), Y);
    axis tight
    
    % 2.3.5 Отображение опорных векторов и подпись
    plot(sv(:, 1), sv(:, 2), 'ko', 'MarkerSize', 10)
    if M == 2
        legend('class 1 region', 'class 2 region', 'class 1', 'class 2', 'Support Vector');
    elseif M == 3
        legend('class 1 region', 'class 2 region', 'class 3 region', ...
                'class 1', 'class 2', 'class 3', 'Support Vector');
    end
    hold off
end   % подпись, если включена визуализация

%% 3. Расчет теоретических матриц вероятностей ошибок распознавания
PIJ = zeros(M);
PIJB = zeros(M);
mg = zeros(M);
Dg = zeros(M);
l0_ = zeros(M);    
for i = 1 : M 
    for j = i+1 : M 
           dmij = m(:, i)-m(:, j); 
           l0_(i, j) = log(pw(j) / pw(i)); 
           dti = det(C(:, :, i));
           dtj = det(C(:, :, j));
           trij = trace(C_(:, :, j) * C(:, :, i) - eye(n));
           trji = trace(eye(n) - C_(:, :, i) * C(:, :, j));
           mg1 = 0.5 * (trij+dmij' * C_(:, :, j) * dmij-log(dti / dtj)); 
           Dg1 = 0.5 * trij ^ 2+dmij' * C_(:, :, j) * C(:, :, i) * C_(:, :, j) * dmij; 
           mg2 = 0.5 * (trji-dmij' * C_(:, :, i) * dmij+log(dtj / dti)); 
           Dg2 = 0.5 * trji ^ 2+dmij' * C_(:, :, i) * C(:, :, j) * C_(:, :, i) * dmij; 
           sD1 = sqrt(Dg1);
           sD2 = sqrt(Dg2);
           PIJ(i, j) = normcdf(l0_(i, j), mg1, sD1);
           PIJ(j, i) = 1 - normcdf(l0_(i, j), mg2, sD2);
           mu2 = (1 / 8) * dmij' * ((C(:, :, i) / 2+C(:, :, j) / 2) ^ -1) * dmij...
               +0.5 * log((dti+dtj) / (2 * sqrt(dti * dtj))); % расстояние Бхатачария
           PIJB(i, j) = sqrt(pw(j) / pw(i)) * exp(-mu2);
           PIJB(j, i) = sqrt(pw(i) / pw(j)) * exp(-mu2); % границы Чернова
    end
    PIJB(i, i) = 1 - sum(PIJB(i, :)); % нижняя граница вероятности правильного распознавания
    PIJ(i, i) = 1 - sum(PIJ(i, :)); % теоретические вероятности правильного распознавания
end

%% 4. Тестирование алгоритма методом статистических испытаний
Pcv = zeros(M); % + инициализация экспериментальной матрицы ошибок

x = ones(n, 1); u = zeros(M, 1);
Pc_ = zeros(M); % экспериментальная матрица вероятностей ошибок из 3й лабы
for k = 1 : K % цикл по числу испытаний
    for i = 1 : M % цикл по классам
        [x, px] = randncor(n, 1, C(:, :, i));
        x = x + m(:, i); % генерация образа i-го класса             
        for j = 1 : M % вычисление значения разделяющих функций из 3й лабы
            u(j) = -0.5 * (x - m(:, j))' * C_(:, :, j) * (x - m(:, j)) - 0.5 * log(det(C(:, :, j)))+log(pw(j));                    
        end
        [ui, iai] = max(u); % определение максимума
        Pc_(i, iai) = Pc_(i, iai)+1; % фиксация результата распознавания

        % Прогон по всем классификаторам SVM
        Scores = []; % массив для фиксации результатов от разных классификаторов
        for j = 1 : M % цикл по классам
            % Вызываем классификатор для сравнения j-го и класса с
            % остальными из массива svm_strs и распознаём им образ x
            [~, score] = predict(svm_strs{j}, x'); % определяем апостериорную плотность для j-го класса
            Scores = [Scores, score(:, 2)]; 
        end
        [~, iai] = max(Scores); % индекс класса = индексу макс. элемента
        Pcv(i, iai) = Pcv(i, iai)+1;	 % фиксация результата распознавания
    end % цикл по классам
end
Pc_ = Pc_ / K;
Pcv = Pcv / K; % + нормировка экспериментальной матрицы по svm на число испытаний 

% Вывести матрицы ошибок (можно без Чернова) для сравнения циферок
disp('Теоретическая матрица вероятностей ошибок');
disp(PIJ);
% disp('Матрица вероятностей ошибок на основе границы Чернова');
% disp(PIJB);
disp('Экспериментальная матрица вероятностей ошибок');
disp(Pc_);
disp('Экспериментальная матрица вероятностей ошибок SVM');
disp(Pcv);
